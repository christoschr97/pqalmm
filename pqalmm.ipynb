{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c71d576-0a18-468b-a3ca-fdf2bae2cfee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bd08c84-2090-47c5-923a-fea0e3be86a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in ./venv/lib/python3.9/site-packages (3.15.5)\n",
      "Requirement already satisfied: typing_extensions>=3.7.4.3 in ./venv/lib/python3.9/site-packages (from pypdf) (4.7.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Users/christoschristodoulou/projects/langchain-gdpr/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting docx2txt\n",
      "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
      "Using legacy 'setup.py install' for docx2txt, since package 'wheel' is not installed.\n",
      "Installing collected packages: docx2txt\n",
      "    Running setup.py install for docx2txt ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed docx2txt-0.8\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Users/christoschristodoulou/projects/langchain-gdpr/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf\n",
    "!pip install docx2txt\n",
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80986113-6714-4989-afbd-5963e727e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file} of type {extension}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file} of type {extension}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print(f'Document format {extension} is not supported')\n",
    "        return None\n",
    "        \n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c3d5a64c-fadb-4051-8118-e166c31925d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load document from wikipedia\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    print(f'Loading from wikipedia documents regarding the query: {query}')\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eda5dc80-b967-4584-a5c3-55c1d1bcb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba14bc94-bbf7-4828-acb4-d45fcabe2d3b",
   "metadata": {},
   "source": [
    "### Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "30d2db29-fedd-4de6-8a40-4609a9f849d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./files/attention.pdf of type .pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state htâˆ’1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', metadata={'source': './files/attention.pdf', 'page': 1})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_document('./files/attention.pdf')\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b318bf96-f858-4225-86e3-07109b3a8f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data)\n",
    "print(len(chunks))\n",
    "print(chunks[10].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72792ae8-ddb6-485d-97aa-d2445f38a276",
   "metadata": {},
   "source": [
    "### Calculate the cost of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "842b4bef-00ce-4b0e-b3f3-01f87b62ba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 10045\n",
      "Embedding Cost in USD: 0.004018\n"
     ]
    }
   ],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.0004:.6f}')\n",
    "    \n",
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77106d26-261b-4521-b4ce-de5aa6d1957b",
   "metadata": {},
   "source": [
    "### Embeding and upload to vector database (PINECONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d627799-08fa-456d-a37e-f707b6a48f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name):\n",
    "    import pinecone\n",
    "    from langchain.vectorstores import Pinecone\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "\n",
    "    if index_name in pinecone.list_indexes():\n",
    "        print(f'Index {index_name} already exists') \n",
    "        print(f'Loading embeddings') \n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "    else:\n",
    "        print(f'Creating {index_name} and embeddings', end='')\n",
    "        pinecone.create_index(index_name, dimension=1536, metric='cosine')\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print(f'Index {index_name} created.')\n",
    "\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2168b01-bcdc-4c6f-a874-d846f19c4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    from langchain.vectorstores import Pinecone\n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "\n",
    "    if index_name == 'all':\n",
    "        indexes = pinecone.list_indexes()\n",
    "        print(f'Delete all indexes')\n",
    "\n",
    "        for index in indexes:\n",
    "            pinecone.delete_index(index)\n",
    "            print(f'Ok..')\n",
    "    else:\n",
    "        print(f'Deleting {index_name} index')\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c3dea79-8ff5-443f-a93b-90b9eeeebb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete all indexes\n",
      "Ok..\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eaa741-0882-44d7-a483-fb99e9ba6cb4",
   "metadata": {},
   "source": [
    "### Create and upload embeddings to the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8ea3511c-4525-41ee-a2eb-b7e51376ea29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating askdocument and embeddingsIndex askdocument created.\n"
     ]
    }
   ],
   "source": [
    "index_name = 'askdocument'\n",
    "vector_store = insert_or_fetch_embeddings(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42231f87-d6c0-485d-a212-608c17571623",
   "metadata": {},
   "source": [
    "### Ask and get answer about a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ffa5de3f-76ff-499b-a0c5-c84286e97d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, query):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    answer = chain.run(query)\n",
    "    return answer\n",
    "\n",
    "def ask_with_memory(vector_store, question, chat_history=[]):\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "\n",
    "    crc = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
    "    result = crc({'question': question, 'chat_history': chat_history})\n",
    "    chat_history.append(question, result['answer'])\n",
    "\n",
    "    return result, chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc3d2f61-ed0c-4892-b526-67168f4bf5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper is titled \"Attention Is All You Need\" and is authored by Ashish Vaswani from Google Brain. The paper is published in the Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL in July 2006. \\n\\nUnfortunately, the provided context does not include a summary or abstract of the paper, so it is unclear what the paper is specifically about.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 'Give me the context of the paper, and what is it about?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ae85d1-6a33-4f47-83cc-45986231acf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Quit or Exit to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #1 test\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ask_and_get_answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mask_and_get_answer\u001b[49m(vector_store, q)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ask_and_get_answer' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i = 1\n",
    "print('Write Quit or Exit to quit.')\n",
    "while True:\n",
    "    q = input(f'Question #{i}')\n",
    "    i = i + 1\n",
    "    if q.lower() in ['quit', 'exit']:\n",
    "        print('Quitting ... bye!')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    answer = ask_and_get_answer(vector_store, q)\n",
    "    print(f'\\nAnswer: {answer}')\n",
    "    print(f'\\n {\"=\" * 50} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a846a0b8-34ac-4a81-8e75-fb1fb4160a94",
   "metadata": {},
   "source": [
    "## We can use wikipedia loader: \n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. we delete the index from any previous embeddings, \n",
    "2. we fetch the wikipedia pages we want based on query and language\n",
    "3. We ask questions in the same manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527bdc85-7fc9-4669-ae40-b92f079efc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd787451-4b2f-4a84-b269-f5e92c79a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_from_wikipedia('Transformer_(machine_learning_model)', 'en')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e661235-70d8-4d06-bb78-02daafa819a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_from_wikipedia('Transformer_(machine_learning_model)', 'en')\n",
    "chunks = chunk_data(data)\n",
    "index = 'attention'\n",
    "vector_store = insert_or_fetch_embeddings(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9963219d-eeb2-4682-88a7-050de342b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'What is transformers'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
